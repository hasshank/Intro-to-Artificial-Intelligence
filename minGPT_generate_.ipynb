{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdMjOzojuJWe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "from mingpt.bpe import BPETokenizer\n",
        "set_seed(3407)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if use_mingpt:\n",
        "  model = GPT.from_pretrained(model_type)\n",
        "else:\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "  model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "model.to(device)\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "rkfc7Ofmuk00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt='', num_samples = 10, steps = 20, do_sample=True):\n",
        "\n",
        "  if use_mingpt:\n",
        "    tokenizer = BPETokenizer()\n",
        "    if prompt == '':\n",
        "      x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
        "    else:\n",
        "      x = tokenizer(prompt).to(device)\n",
        "  else:\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
        "    if prompt == '':\n",
        "      prompt = '<|endoftext|>'\n",
        "    encoded_input = tokenizer(prompt, return_tensors = 'pt').to(device)\n",
        "    x = encoded_input['input_ids']\n",
        "\n",
        "  x = x.expand(num_samples, -1)\n",
        "\n",
        "  y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k = 40)\n",
        "\n",
        "  for i in range(num_samples):\n",
        "    out = tokenizer.decode(y[i].cpu().squeeze())\n",
        "    print('-'*80)\n",
        "    print(out)"
      ],
      "metadata": {
        "id": "LSKl9zeMuzks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(prompt='...', num_samples=10, steps=20)"
      ],
      "metadata": {
        "id": "a4YW8aEvwAjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}